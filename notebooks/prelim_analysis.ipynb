{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/high_spd_work/download_twitter_disease_data/venv/lib/python3.4/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/high_spd_work/download_twitter_disease_data/')\n",
    "from prelim_analysis import (\n",
    "    MakeIter,\n",
    "    twt_generator,\n",
    "    twts,\n",
    "    len_iterable,\n",
    "    preprocess_twt,\n",
    "    eng_stopwords,\n",
    "    cleaned_sentences,\n",
    "    make_model,\n",
    "    flatten_list,\n",
    "    get_word_freq,\n",
    "    plot_map,\n",
    "    get_weeknum_from_filename,\n",
    "    count_related_words_normalized,\n",
    "    make_heatmap_w2vrelated,\n",
    "    scikit_pca,\n",
    "    make_histogram)\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preliminary analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the Keys:\n",
      "dict_keys(['text', 'in_reply_to_user_id_str', 'weeknum', 'coordinates', 'id', 'lang', 'is_quote_status', 'truncated', 'contributors', 'in_reply_to_screen_name', 'in_reply_to_status_id_str', 'filter_level', 'in_reply_to_status_id', 'id_str', 'retweeted', 'in_reply_to_user_id', 'place', 'entities', 'timestamp_ms', 'user', 'created_at', 'favorite_count', 'geo', 'favorited', 'retweet_count', 'source'])\n",
      "\n",
      " Number of english tweets:\n",
      "2305856\n"
     ]
    }
   ],
   "source": [
    "# important keys are 'text', 'created_at', 'coordinates'\n",
    "print(\"These are the Keys:\")\n",
    "for twt in twts:\n",
    "    print(twt.keys())\n",
    "    break\n",
    "\n",
    "# more than 2 million tweets or >8 Gigs\n",
    "print(\"\\n Number of english tweets:\")\n",
    "print(len_iterable(twts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example text:\n",
      "@Walgreens Flu Shot, washing hands frequently, and getting proper amt. of rest.\n",
      "example sentence:\n",
      "['walgreens', 'flu', 'shot', 'washing', 'hands', 'frequently', 'getting', 'proper', 'amt']\n"
     ]
    }
   ],
   "source": [
    "# Train word2vec on cleaned_sentences\n",
    "print(\"example text:\")\n",
    "for twt in twts:\n",
    "    print(twt['text'])\n",
    "    break\n",
    "\n",
    "print(\"example sentence:\")\n",
    "for sentence in cleaned_sentences:\n",
    "    print(sentence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now train\n",
    "model = make_model(cleaned_sentences,\n",
    "                   \"word\",\n",
    "                  size=100, # dimension of word vecs\n",
    "                  window=5, # context size\n",
    "                  min_count=100, #words repeated less than this are discarded\n",
    "                  workers=6 # number of threads\n",
    "                  )\n",
    "\n",
    "# print word to vec results over all tweets for words related to 'hiv'\n",
    "rel_wds = model.most_similar(positive=['hiv'], topn=10)\n",
    "print('\\n most similar word(s) to hiv (according to word2vec):')\n",
    "print(rel_wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print top 30 word frequencies (counts) over all tweets\n",
    "print(\"\\n Word Frequency:\")\n",
    "word_dict, words, counts = get_word_freq(cleaned_sentences)\n",
    "for i in range(30):\n",
    "    print(words[i], counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot all the tweets\n",
    "print(\"\\n Number of tweets with coordinates:\")\n",
    "print(len(coord_twts))\n",
    "plot_map(coord_twts, title='plot all the tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display and plot\n",
    "count_word_week = count_related_words_normalized(rel_wds, \"twt_data/\")\n",
    "display(count_word_week)\n",
    "count_word_week.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering of words, using word2vec distance measure (cosine similarity)\n",
    "(using word2vec vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make heatmap (distance between top 10 word-vectors related to 'hiv'\n",
    "# over all tweets)\n",
    "make_heatmap_w2vrelated(model, rel_wds=rel_wds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Do spectral clustering of word-vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA (visualization of all words in 2D)\n",
    "X, explained_variances = scikit_pca(model, cluster=\"kmeans\")\n",
    "print(\"explained variance ratio: \")\n",
    "print(explained_variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make histogram of word-vec similarity pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_histogram(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ok, now lets look at phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = make_model(cleaned_sentences,\n",
    "                   word_type = \"phrase\"\n",
    "                  size=100, # dimension of word vecs\n",
    "                  window=5, # context size\n",
    "                  min_count=100, #words repeated less than this are discarded\n",
    "                  workers=6 # number of threads\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
