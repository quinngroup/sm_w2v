{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM (Figures 1, 2 and Tables 1, 2)\n",
    "\n",
    "This is adapted (partially) from https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/dtm_example.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, utils\n",
    "from gensim.models.wrappers.dtmmodel import DtmModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train = False\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# script author defined stop words\n",
    "STOPWORDS_ = ['rt', '', 'httpst', 'amp']\n",
    "\n",
    "# load the tweets\n",
    "documents = []\n",
    "time_seq = []\n",
    "weeknum = ''\n",
    "with open('../data/c_twitter.json') as f_in:\n",
    "    for i, l in enumerate(f_in):\n",
    "        c_twt = json.loads(l)\n",
    "        \n",
    "        # remove punctuation and stopwords for this analysis\n",
    "        words = re.sub(r'[^a-zA-Z\\s]', '', c_twt['text'])\n",
    "        words = words.split()\n",
    "        words = [w for w in words if (w not in STOPWORDS and w not in STOPWORDS_)]\n",
    "        words = [bytes(w, 'utf-8') for w in words]\n",
    "        \n",
    "        # c_twt: {'weeknum': str, 'c_text': str, 'tags': [str]}\n",
    "        documents.append(words)\n",
    "        \n",
    "        if weeknum != c_twt['weeknum']:\n",
    "            time_seq.append(i)\n",
    "            weeknum = c_twt['weeknum']\n",
    "            \n",
    "time_seq = time_seq[1:]\n",
    "time_seq.append(len(documents))\n",
    "\n",
    "acc = 0\n",
    "for i in range(1, len(time_seq)):\n",
    "    acc = acc + time_seq[i-1]\n",
    "    time_seq[i] = time_seq[i] - acc\n",
    "\n",
    "print(time_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.3, keep_n=100000)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model (or load model from file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/high_spd_work/sm_w2v/tools/dtm/dtm/dtm', '--ntopics=10', '--model=dtm', '--mode=fit', '--initialize_lda=true', '--corpus_prefix=/tmp/fe9b33_train', '--outname=/tmp/fe9b33_train_out', '--alpha=0.01', '--lda_max_em_iter=10', '--lda_sequence_min_iter=6', '--lda_sequence_max_iter=20', '--top_chain_var=0.005', '--rng_seed=0']' returned non-zero exit status -9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-29ebb8c54b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdtm_binary_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/high_spd_work/sm_w2v/tools/dtm/dtm/dtm'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     model = DtmModel(dtm_binary_path, corpus, time_seq, num_topics=10,\n\u001b[1;32m----> 6\u001b[1;33m                      id2word=dictionary, initialize_lda=True, rng_seed=0)\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../models/dtm_example.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/wrappers/dtmmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dtm_path, corpus, time_slices, mode, model, num_topics, id2word, prefix, lda_sequence_min_iter, lda_sequence_max_iter, lda_max_em_iter, alpha, top_chain_var, rng_seed, initialize_lda)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_slices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfout_liklihoods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/wrappers/dtmmodel.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus, time_slices, mode, model)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtm_path\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Running command %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mem_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfem_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m   1161\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m             \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1163\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1164\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['/high_spd_work/sm_w2v/tools/dtm/dtm/dtm', '--ntopics=10', '--model=dtm', '--mode=fit', '--initialize_lda=true', '--corpus_prefix=/tmp/fe9b33_train', '--outname=/tmp/fe9b33_train_out', '--alpha=0.01', '--lda_max_em_iter=10', '--lda_sequence_min_iter=6', '--lda_sequence_max_iter=20', '--top_chain_var=0.005', '--rng_seed=0']' returned non-zero exit status -9"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    dtm_binary_path = '/high_spd_work/sm_w2v/tools/dtm/dtm/dtm'\n",
    "    model = DtmModel(dtm_binary_path, corpus, time_seq, num_topics=10,\n",
    "                     id2word=dictionary, initialize_lda=True, rng_seed=0)\n",
    "    model.save('../models/dtm_example.model')\n",
    "else:\n",
    "    model = DtmModel.load('../models/doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = model.show_topic(topicid=3, time=11, topn=20)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1, 2 and Table 1, 2: Dynamic Topic Modeling\n",
    "\n",
    "I'm proposing that in figures 1 and 2, we pick 2 topics from the Dynamic Topic Model, and describe the progression in time of these 2 topics with time series graphs, and tables 1 and 2 will be top 10 tables of these same topics over time\n",
    "\n",
    "\n",
    "Topic ID's 8, 7 and 3 are all interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell just gets the week nums (YYYY-WW)\n",
    "def weeknums():\n",
    "    # first  week is 2015-47\n",
    "    week = 47\n",
    "    year = 2015\n",
    "    weeknums = []\n",
    "    for i in range(len(model.time_slices)):\n",
    "        weeknums.append(str(year) + '-' + str(week))\n",
    "        week =+ 1\n",
    "\n",
    "        if week > 53:\n",
    "            year =+ 1\n",
    "            week = 1\n",
    "    return weeknums\n",
    "\n",
    "weeknums = weeknums()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1: top 10 words associated with topic ID 3 over time\n",
    "\n",
    "note how 'prep' is absent from top ten in the beginning, then increases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t_index in range(len(model.time_slices)):\n",
    "    print(\"--- time slice: \", t_index, \"\\n\")\n",
    "    print(pd.DataFrame(model.show_topic(topicid=3, time=t_index, topn=10), columns=['Probability', 'Word']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1: key terms associated with ID 3 over time\n",
    "\n",
    "note how 'prep' is absent from top ten in the beginning, then increases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# needs to be re-run. Use tf-idf\n",
    "data = []\n",
    "for t_index in range(len(model.time_slices)):\n",
    "    words = model.show_topic(topicid=3, time=t_index, topn=1000)\n",
    "        \n",
    "    columns = ['prep', 'hiv', 'warning', 'worse', 'deadly', 'disease']\n",
    "    row = []\n",
    "    for col in columns:\n",
    "        cell = [w[0] for w in words if w[1] == col]\n",
    "        if not cell:\n",
    "            cell = .000001\n",
    "        else:\n",
    "            cell = cell[0]\n",
    "        row.append(cell)\n",
    "    data.append(row)\n",
    "    \n",
    "df = pd.DataFrame(data, columns=columns, index=weeknums)\n",
    "ax = df.plot(title='Fig1: Time Series of DTM topic 3')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Fig1: Time Series of DTM topic 3.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: top 10 words associated with topic ID 8 over time\n",
    "Note how at time points 2 and 3 we get world AIDS day related words. At time points 7, 8 and 9 the antibiotic 'Zithromax' appears, then disappears. 'gay' also seems to come up during the later time points, which parallels the #egaylity in the Doc2Vec results. 'rt' is not entirely interesting, since it is the name of a news agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t_index in range(len(model.time_slices)):\n",
    "    print(\"--- time slice: \", t_index, \"\\n\")\n",
    "    print(pd.DataFrame(model.show_topic(topicid=8, time=t_index, topn=10), columns=['Probability', 'Word']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# needs to be re-run. Use tf-idf\n",
    "data = []\n",
    "for t_index in range(len(model.time_slices)):\n",
    "    words = model.show_topic(topicid=8, time=t_index, topn=1000)\n",
    "        \n",
    "    columns = ['hiv', 'aids', 'world', 'day', 'zithromax', 'chlamydia', 'gay']\n",
    "    row = []\n",
    "    for col in columns:\n",
    "        cell = [w[0] for w in words if w[1] == col]\n",
    "        if not cell:\n",
    "            cell = .000001\n",
    "        else:\n",
    "            cell = cell[0]\n",
    "        row.append(cell)\n",
    "    data.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns, index=weeknums)\n",
    "ax = df.plot(title='Fig2: Time Series of DTM topic 8')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('Fig2: Time Series of DTM topic 8.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
