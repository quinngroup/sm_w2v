{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec (Figures 3, 4 and Tables 3, 4)\n",
    "\n",
    "adapted (partially) from https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "import gensim\n",
    "\n",
    "# script author (me) defined stop words\n",
    "STOPWORDS_ = ['re', '-', '', 'httpst']\n",
    "\n",
    "Document = namedtuple('Document', 'words tags')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "with open('../data/c_twitter.json') as f_in:\n",
    "    for line in f_in:\n",
    "        c_twt = json.loads(line)\n",
    "        # c_twt: {'weeknum': str, 'c_text': str, 'tags': [str]}\n",
    "        words = c_twt['c_text'].split()\n",
    "        words = [w for w in words if w not in STOPWORDS_]\n",
    "        tags = [c_twt['id']] + c_twt['tags']\n",
    "        alldocs.append(Document(words, tags))\n",
    "        \n",
    "shuffle_docs = alldocs[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(words=['hiv', 'is', 'not', 'your', 'shame', '.', 'if', 'you', 'are', 'positive', ',', 'make', '#', 'hiv', 'be', 'ypur', 'strenghth', 'and', 'your', 'platform', 'to', 'be', 'a', 'more', 'healthy', 'you', '.', 'be', 'inspired', '.'], tags=[667358327412797441, 'LGeorgeBTQ-*-225970038', '#hiv'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array is too big.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-debc9d072507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# currently running out of memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, sentences, keep_raw_vocab, trim_rule)\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# initial survey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# build tables & arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mfinalize_vocab\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# set initial input/projection and hidden weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msort_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"using concatenative %d-dimensional layer1\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/high_spd_work/sm_w2v/venv/lib/python3.4/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mreset_weights\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctag_syn0_lockf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# zeros suppress learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array is too big."
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "# PV-DM w/average\n",
    "model= Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores,\n",
    "           max_vocab_size=10000, seed=1)\n",
    "\n",
    "\n",
    "# currently running out of memory\n",
    "model.build_vocab(alldocs)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Doc2Vec training could be a figure??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "passes = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(shuffle_docs)\n",
    "    model.train(shuffle_docs)\n",
    "    print(epoch, model.most_similar('hiv', topn=10))\n",
    "    \n",
    "    # do some validation after each epoch\n",
    "    doc_ids = random.sample(range(len(alldocs)), 1000)\n",
    "    score = 0\n",
    "    for doc_id in doc_ids:\n",
    "        inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "        score += sum(abs(model.docvecs[doc_id] - inferred_docvec))\n",
    "    print(\"--- score: \", score)\n",
    "    print(\"ELAPSED TIME (seconds): \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do visualization/clustering of users\n",
    "# do visualization/clustering of hashtags\n",
    "# do visualization/clustering of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 4: Related Word-Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_vecs = model.most_similar(['hivaids'], topn=10)\n",
    "df_hivaids = pd.DataFrame(data=new_vecs, columns=['Related-Word', 'Cosine Similarity to \\\"hivaids\\\"'])\n",
    "df_hivaids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.most_similar(['prep'], topn=10)\n",
    "df_prep = pd.DataFrame(data=new_vecs, columns=['Related Word', 'Cosine Similarity to \\\"prep\\\"'])\n",
    "df_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.most_similar(['prophylaxis'], topn=10)\n",
    "df_prophylaxis = pd.DataFrame(data=new_vecs, columns=['Related Word', 'Cosine Similarity to \\\"prophylaxis\\\"'])\n",
    "df_prophylaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.most_similar(['truvada'], topn=10)\n",
    "df_truvada = pd.DataFrame(data=new_vecs, columns=['Related Word', 'Cosine Similarity to \\\"truvada\\\"'])\n",
    "df_truvada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3: PCA and tSNE plots of relevent word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# append path to my module\n",
    "if '/high_spd_work/sm_w2v' not in sys.path:\n",
    "    sys.path.append('/high_spd_work/sm_w2v')\n",
    "from sm_w2v.plot_utils import scatter_plot\n",
    "\n",
    "# related words from above\n",
    "related_words = list(df_hivaids.iloc[:,0].values) + \\\n",
    "                list(df_prep.iloc[:,0].values) + \\\n",
    "                list(df_prophylaxis.iloc[:,0].values) + \\\n",
    "                list(df_truvada.iloc[:,0].values)\n",
    "\n",
    "# Prepare data matrix\n",
    "X = []\n",
    "text_annotations = []\n",
    "for word in model.vocab:\n",
    "    X.append(model[word])\n",
    "    if word in related_words:\n",
    "        text_annotations.append(word)\n",
    "    else:\n",
    "        text_annotations.append(\"\")\n",
    "X = np.array(X)\n",
    "\n",
    "# Do k-means on original data matrix\n",
    "kmeans = KMeans(n_clusters=3, random_state=1)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Do PCA\n",
    "pca = PCA(n_components=2, copy=True)\n",
    "pca.fit(X)\n",
    "print(\"PCA explained variance ratio: \", pca.explained_variance_ratio_)\n",
    "pca_comps = pca.transform(X)\n",
    "\n",
    "# Do tSNE\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "tsne_comps = tsne.fit_transform(X)\n",
    "\n",
    "# Scatter plot\n",
    "rand_seed = 0\n",
    "alpha_high = 1.0\n",
    "alpha_low = 0.05\n",
    "down_samp_rate = 0.1\n",
    "plot_lims = None\n",
    "\n",
    "# save PCA plot in 'notebooks' directory\n",
    "scatter_plot(pca_comps[:,0], pca_comps[:,1], alpha_high, alpha_low,\n",
    "            kmeans_labels, text_annotations,\n",
    "            down_samp_rate, \"Fig3a: PCA of Related Words\", rand_seed, [-5,10,-20,10])\n",
    "\n",
    "# save tSNE plot in 'notebooks' directory\n",
    "scatter_plot(tsne_comps[:,0], tsne_comps[:,1], alpha_high, alpha_low,\n",
    "            kmeans_labels, text_annotations,\n",
    "            down_samp_rate, \"Fig3b: tSNE of Related Words\", rand_seed, [-15,3,-8,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 4: Related Hashtags and Tweets\n",
    "\n",
    "Hashtags begin with \"#\", users begin with \"user--\" and tweets are just a number - that refers to the corresponding index in the \"alldocs\" variable, or the \"../data/c_twitter.json\" cleaned tweets file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_vecs = model.docvecs.most_similar(['#truvada'], topn=10)\n",
    "df_hash_truvada = pd.DataFrame(data=new_vecs, columns=['Related Hashtag/User/Tweet', 'Cosine Similarity to \\\"#truvada\\\"'])\n",
    "df_hash_truvada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.docvecs.most_similar(['#sexwork'], topn=10)\n",
    "df_hash_sexwork = pd.DataFrame(data=new_vecs, columns=['Related Hashtag/User/Tweet', 'Cosine Similarity to \\\"#sexwork\\\"'])\n",
    "df_hash_sexwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.docvecs.most_similar(['#prep'], topn=10)\n",
    "df_hash_prep = pd.DataFrame(data=new_vecs, columns=['Related Related Hashtag/User/Tweet', 'Cosine Similarity to \\\"#prep\\\"'])\n",
    "df_hash_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note, this tweet is popular, warning about 13 signs which indicate that you need HIV testing.\n",
    "# We see evidence of this populat tweet-retweet in both DTM and Doc2Vec:\n",
    "#\n",
    "# Document(words=['#', 'krtebireysyle', 'if', 'you', 'see', 'this', '13',\n",
    "# 'symptoms', '.', 'do', 'hiv', 'test', 'immediately', '.', 'please', 'read'],\n",
    "# tags=[603177, '#KFB_Mz_Sope', 'kürtçebirşeysöyle'])\n",
    "#\n",
    "alldocs[603177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_vecs = model.docvecs.most_similar(['#imtesting'], topn=10)\n",
    "df_hash_imtesting = pd.DataFrame(data=new_vecs, columns=['Related Related Hashtag/User/Tweet', 'Cosine Similarity to \\\"#imtesting\\\"'])\n",
    "df_hash_imtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4: Related Hashtags and Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# related words from above\n",
    "related_words = list(df_hash_truvada.iloc[:,0].values) + \\\n",
    "                list(df_hash_sexwork.iloc[:,0].values) + \\\n",
    "                list(df_hash_prep.iloc[:,0].values) + \\\n",
    "                list(df_hash_imtesting.iloc[:,0].values)\n",
    "\n",
    "# Prepare data matrix\n",
    "X = []\n",
    "text_annotations = []\n",
    "for i, word in enumerate(model.docvecs.doctags):\n",
    "    if (i % 100 == 0 or word in related_words) :\n",
    "        X.append(model.docvecs[word])\n",
    "        if word in related_words:\n",
    "            text_annotations.append(word)\n",
    "        else:\n",
    "            text_annotations.append(\"\")\n",
    "X = np.array(X)\n",
    "\n",
    "# Do k-means on original data matrix\n",
    "kmeans = KMeans(n_clusters=3, random_state=1)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Do PCA\n",
    "pca = PCA(n_components=2, copy=True)\n",
    "pca.fit(X)\n",
    "print(\"PCA explained variance ratio: \", pca.explained_variance_ratio_)\n",
    "pca_comps = pca.transform(X)\n",
    "\n",
    "# Do tSNE\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "tsne_comps = tsne.fit_transform(X)\n",
    "\n",
    "# Scatter plot\n",
    "rand_seed = 0\n",
    "alpha_high = 1.0\n",
    "alpha_low = 0.05\n",
    "down_samp_rate = 0.1\n",
    "plot_lims = None\n",
    "\n",
    "# save PCA plot in 'notebooks' directory (it won't show here in the notebook)\n",
    "scatter_plot(pca_comps[:,0], pca_comps[:,1], alpha_high, alpha_low,\n",
    "            kmeans_labels, text_annotations,\n",
    "            down_samp_rate, \"Fig4a: PCA of Related Hashtags-Users-Tweets\", rand_seed, [0,25,-7,3])\n",
    "\n",
    "# save tSNE plot in 'notebooks' directory (it won't show here in the notebook)\n",
    "scatter_plot(tsne_comps[:,0], tsne_comps[:,1], alpha_high, alpha_low,\n",
    "            kmeans_labels, text_annotations,\n",
    "            down_samp_rate, \"Fig4b: tSNE of Hashtags-Users-Tweets\", rand_seed, [-10,5,-8,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
